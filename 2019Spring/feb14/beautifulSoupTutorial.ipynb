{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Study Group (PSG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will be reviewing the python package [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/ \"BeautfulSoup Homepage\")\n",
    "\n",
    "1. We will explore applying this library to parse an [xml](https://en.wikipedia.org/wiki/XML \"definition\") file downloaded from [NCBI's BLAST program](https://blast.ncbi.nlm.nih.gov/Blast.cgi \"BLAST Homepage\").\n",
    "2. Finally, we will see how coupling this with the [requests](http://docs.python-requests.org/en/master/ \"Requests Homepage\") library will provide most of the tools required for building a generic web-crawler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "\n",
    "#### Get the Data\n",
    "\n",
    "The data for this tutorial is available [here](https://github.com/ComBEE-UW-Madison/PythonStudyGroup.git \"COMBEE GitHub\")\n",
    "\n",
    "You may clone the repo and follow along with the command:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/ComBEE-UW-Madison/PythonStudyGroup.git\n",
    "```\n",
    "\n",
    "Navigate to the COMBEE github data directory.\n",
    "\n",
    "`cd /Path/to/PythonStudyGroup/2019Spring/data`\n",
    "\n",
    "\n",
    "#### Pip installing BeautifulSoup and requests\n",
    "\n",
    "First we will ensure we have the BeautifulSoup library with `pip`\n",
    "\n",
    "```bash\n",
    "pip install --user beautifulsoup\n",
    "pip install --user requests\n",
    "```\n",
    "\n",
    "The `--user` flag will install the library in your own `$HOME`. (This will allow us to bypass requiring super user permissions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our BeautifulSoup object\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a BLAST output in xml in our `$HOME/COMBEE/data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = 'markup.xml'\n",
    "fp = 'BLASTalignment.xml'\n",
    "fh = open(fp)\n",
    "soup = BeautifulSoup(fh, features='xml')\n",
    "fh.close()\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(soup)\n",
    "# [c for c in soup.children]\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.BlastOutput_program.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "children = list()\n",
    "for child in soup.children:\n",
    "    children.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('gene A',3), ('gene B',3), ('gene C',5)]\n",
    "d = {elem[0].replace('gene ','transcript'):elem[1] for elem in l}\n",
    "# d = {elem[0].replace('gene ','transcript') for elem in l}    \n",
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = [child for child in soup.children]\n",
    "# print(children)\n",
    "seq = soup.Hsp_hseq.text\n",
    "# print(seq)\n",
    "print(soup.Hit_accession.string)\n",
    "print(soup.Hsp_identity.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(children)\n",
    "tags = [tag for tag in soup.find_all(True)]\n",
    "print(len(tags))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = [hit_acc.string for hit_acc in soup.find_all('Hit_accession')]\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [c for c in soup.descendants]\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### An example of a web-crawler (spider)\n",
    "\n",
    "Here we use...\n",
    "\n",
    "- requests\n",
    "- random\n",
    "- string\n",
    "- multiprocessing\n",
    "- BeautifulSoup \n",
    "\n",
    "to quickly request and parse HTML markups to retrieve links for a recursive link explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "import requests as r\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to check a library's methods\n",
    "# dir(random)\n",
    "# dir(string)\n",
    "# dir(r)\n",
    "# dir(BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we create a function to generate a random url\n",
    "def random_url(url_length=3):\n",
    "    #Starting three characters (lowercase)\n",
    "    starting = ''.join(random.choice(string.ascii_lowercase) for _ in range(url_length))\n",
    "    url = ''.join(['http://', starting, '.com'])\n",
    "    return(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    url = random_url(i+2)\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_local_links(url,link):\n",
    "    # we need to handle for local links need to prepend url for requests\n",
    "    if link.startswith('/'):\n",
    "        return(''.join([url,link]))\n",
    "    else:\n",
    "        return(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    try:\n",
    "        response = r.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        # Hopes of avoiding navigation bars\n",
    "        body = soup.body\n",
    "        links = [link.get('href') for link in body.find_all('a')]\n",
    "        links = [handle_local_links(url, link) for link in links]\n",
    "        links = [str(link.encode('ascii')) for link in links]\n",
    "        return(links)\n",
    "    except TypeError as e:\n",
    "        print(e)\n",
    "        return([])\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        return([])\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        return([])\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic for scraping the web.\n",
    "\n",
    "I've provided a simple example. Perhaps this could be applied to selecting a random organism from NCBI.\n",
    "\n",
    "Pseudocode:\n",
    "```\n",
    "urls = random_urls()\n",
    "While num_urls_found < 100:\n",
    "    markups = [retrieve_url_markup(url) for url in urls]\n",
    "    urls = [url for get_urls(markup) for markup in markups for get_urls(marku]\n",
    "    num_urls_found += len(urls)\n",
    "```\n",
    "\n",
    "### Follow-up\n",
    "\n",
    "How would this be performed? What markups would need to be retrieved and how could you parse them to access the organism's genome?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    num_searches = 50\n",
    "    p = Pool(processes=num_searches)\n",
    "    parse_us = [random_url() for _ in range(num_searches)]\n",
    "    while True:\n",
    "        data = p.map(get_links, [link for link in parse_us])\n",
    "    #     Flatten data\n",
    "        data = [url for url_list in data for url in url_list]\n",
    "#         for url_list in data:\n",
    "#             for url in url_list:\n",
    "#                 data.append(url)\n",
    "        parse_us = data\n",
    "        p.close()\n",
    "\n",
    "        with open('urls.text', 'w') as fh:\n",
    "            fh.write(str(data))\n",
    "            print('Written {}'.format(fh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = r.get('https://www.crummy.com/software/BeautifulSoup/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.text\n",
    "soup = BeautifulSoup(resp.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
